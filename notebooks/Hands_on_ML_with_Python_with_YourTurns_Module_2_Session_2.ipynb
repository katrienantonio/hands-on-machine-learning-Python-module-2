{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> <span style=\"color:black\"> Hands-on Machine Learning with Python  </h1> </center> \n",
        "<center> <h2> <span style=\"color:red\"> Module 2: Tree-based machine learning methods </h1> </center>\n",
        "<center> <h3> <span style=\"color:red\"> Session 2: Ensembles </h1> </center>"
      ],
      "metadata": {
        "id": "UWk6ioM5BZKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structure of the notebook\n",
        "\n",
        "* [Chapter 1 - Introduction](#one)\n",
        "    + [1.1 Objectives of the notebook](#one-one)\n",
        "    + [1.2 Library requirements](#one-two)\n",
        "\n",
        "* [Chapter 2 - Bagging](#two)\n",
        "    + [2.1 DIY example](#two-one)\n",
        "    + [2.2 Regression](#two-two)\n",
        "    + [2.3 Out-of-bag](#two-three)\n",
        "    + [2.4 Grid search CV](#two-four)\n",
        "        \n",
        "* [Chapter 3 - Random forest](#three)\n",
        "    + [3.1 Dominant features](#three-one)\n",
        "    + [3.2 Feature sampling](#three-two)\n",
        "    + [3.3 Tuning & insights](#three-three)\n",
        "\n",
        "* [Chapter 4 - Boosting](#four)\n",
        "    + [4.1 Parameters](#four-one)\n",
        "    + [4.2 Out-of-bag](#four-two)\n",
        "    + [4.3 Early stopping](#four-three)\n",
        "    + [4.4 Other implementations](#four-four)\n",
        "\n",
        "* [Chapter 5 - XGBoost](#five)\n",
        "    + [5.1 Claim frequency](#five-one)\n",
        "    + [5.2 Claim severity](#five-two)\n",
        "    + [5.3 Random search CV](#five-three)\n"
      ],
      "metadata": {
        "id": "rLwSpISOFdnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1 - Introduction <a name=\"one\"></a>"
      ],
      "metadata": {
        "id": "wkKSGnBcFdaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Objectives of the notebook <a name=\"one-one\"></a>\n",
        "The objectives of this notebook are to:\n",
        "1. Build tree-based ensembles for typical regression, classification and actuarial problems.\n",
        "1. Tune the parameters of tree-based ensembles to obtain optimal performance.\n",
        "1. Inspect ensembles to gain insights in the underlying decision process."
      ],
      "metadata": {
        "id": "NgXpTxuHFxre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Library requirements <a name=\"one-two\"></a>\n",
        "We start by importing all the required Python packages for this notebook."
      ],
      "metadata": {
        "id": "St-Z1wDnF5ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from plotnine import ggplot, geom_point, geom_line, geom_vline, geom_hline, aes, theme_set, theme_bw\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.inspection import permutation_importance, partial_dependence, PartialDependenceDisplay\n",
        "from sklearn.ensemble import BaggingRegressor, BaggingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "# set the black and white theme for ggplot to get rid of gray backgrounds\n",
        "theme_set(theme_bw())"
      ],
      "metadata": {
        "id": "iitbIysoCrcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2 - Bagging <a name=\"two\"></a>\n",
        "Bagging stands for **b**ootstrap **agg**regat**ing** and is a technique that can be applied to different types of base learners.  We will focus on decision trees as base learners."
      ],
      "metadata": {
        "id": "dsR2TjadCJfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 DIY example <a name=\"two-one\"></a>\n",
        "We start by showcasing a small DIY experiment where we perform the two steps in bagging ourself:\n",
        "1. bootstrap: sample the data with replacement.\n",
        "2. aggregating: combine the models/predictions.\n",
        "\n",
        "The goal is to understand the principles of how bagging models are set up.."
      ],
      "metadata": {
        "id": "R6hRMlE1JpTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We simulate data from a sinusoidal pattern with some normally distributed noise on top of it:"
      ],
      "metadata": {
        "id": "wyWCxSBRLBp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMFeHHbiBNG4"
      },
      "outputs": [],
      "source": [
        "# set a seed for reproducibility\n",
        "np.random.seed(5678)\n",
        "# generate a x array from 0 to 2*pi\n",
        "x = np.linspace(start=0, stop=2*math.pi, num=500)\n",
        "# generate the true model m as the sin of x\n",
        "m = np.sin(x)\n",
        "# generate the observed y by adding normal noise to m\n",
        "y = m + np.random.normal(loc=0, scale=0.5, size = len(m))\n",
        "# collect the arrays in a dataframe\n",
        "dfr = pd.DataFrame.from_dict({'x':x, 'm':m, 'y':y})\n",
        "# print the dataframe\n",
        "dfr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simulated data (gray points) and the underlying true model (green line) look as follows:"
      ],
      "metadata": {
        "id": "45ocnm6KMA4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot simulated data\n",
        "ggplot(dfr, aes(x = 'x')) + geom_point(aes(y = 'y'), alpha = 0.3) + geom_line(aes(y = 'm'), colour = 'darkgreen', size = 1.5)"
      ],
      "metadata": {
        "id": "nVJrD_DjMAdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create our first bootstrap sample by taking observations at random with replacement via the `random.choices` function:"
      ],
      "metadata": {
        "id": "z-aCj51DhOMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check out the difference between sample and choices\n",
        "random.seed(12345)\n",
        "print(random.choices(range(5),k=5))\n",
        "print(random.sample(range(5),k=5))"
      ],
      "metadata": {
        "id": "uFItYLNyOXQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the number of rows\n",
        "nrow_dfr = dfr.shape[0]\n",
        "# set a seed for reproducibility\n",
        "random.seed(1234)\n",
        "# get a bootstrap sample with replacement\n",
        "id1 = random.choices(range(nrow_dfr), k=int(0.8*nrow_dfr))\n",
        "# subset the observations\n",
        "dfr_b1 = dfr.iloc[id1]\n",
        "# show the dataframe\n",
        "dfr_b1.sort_values('x')"
      ],
      "metadata": {
        "id": "ul6g3hfceOx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do the exact same steps again, but now with a different seed to obtain a different sample:"
      ],
      "metadata": {
        "id": "upTh5kYkhd75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set a seed for reproducibility\n",
        "random.seed(5678)\n",
        "# get a bootstrap sample with replacement\n",
        "id2 = random.choices(range(nrow_dfr), k=int(0.8*nrow_dfr))\n",
        "# subset the observations\n",
        "dfr_b2 = dfr.iloc[id2]\n",
        "# show the dataframe\n",
        "dfr_b2.sort_values('x')"
      ],
      "metadata": {
        "id": "m9Mk9BvDggEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating our bootstrap datasets, we now fit a decision tree to each sample of the data:"
      ],
      "metadata": {
        "id": "-G_cimQ3RqFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a deep DecisionTreeRegressor to dfr_b1\n",
        "tree_bag1 = DecisionTreeRegressor(criterion='squared_error', max_depth=5, min_samples_leaf=3).fit(dfr_b1.x.to_numpy().reshape(-1, 1), dfr_b1.y.to_numpy())\n",
        "# fit a deep DecisionTreeRegressor to dfr_b2\n",
        "tree_bag2 = DecisionTreeRegressor(criterion='squared_error', max_depth=5, min_samples_leaf=3).fit(dfr_b2.x.to_numpy().reshape(-1, 1), dfr_b2.y.to_numpy())"
      ],
      "metadata": {
        "id": "7P3JjZNTRpvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fitting each tree, we now make predictions and aggregate them by taking the average for each observation over both trees:"
      ],
      "metadata": {
        "id": "x7Tcz1hmOmIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get original x values\n",
        "preds_bag = pd.DataFrame({'x':dfr.x})\n",
        "# predictions for first tree\n",
        "preds_bag['pred_bag1'] = tree_bag1.predict(dfr.x.to_numpy().reshape(-1,1))\n",
        "# predictions for second tree\n",
        "preds_bag['pred_bag2'] = tree_bag2.predict(dfr.x.to_numpy().reshape(-1,1))\n",
        "# average both predictions \n",
        "preds_bag['pred_mean'] = (preds_bag['pred_bag1'] + preds_bag['pred_bag2']) / 2\n",
        "# inspect the dataframe\n",
        "preds_bag"
      ],
      "metadata": {
        "id": "0UuqcQabRM2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For plotting purposes we transform the data from a wide to a long format via the `pandas.melt` function:"
      ],
      "metadata": {
        "id": "xPqZw_YOx8oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# melt to long format\n",
        "preds_bag_long = pd.melt(preds_bag, id_vars=['x'], var_name='model', value_name='pred')"
      ],
      "metadata": {
        "id": "oHXa7vQXxV7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now plot the predictions for both individual trees and the aggregated prediction from our bagged model:"
      ],
      "metadata": {
        "id": "M2HaHulhxVfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot predictions\n",
        "ggplot(preds_bag_long, aes(x = 'x')) + geom_line(aes(y = 'pred', colour = 'model'), size = 1, alpha = 0.5)"
      ],
      "metadata": {
        "id": "wF2npNYNvA1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the averaged prediction flattens some of the extreme predictions by one of the individual trees. Let's take this one step further."
      ],
      "metadata": {
        "id": "KGNXosBSyyoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Create a third bootstrap sample and fit a decision tree to it. Be sure to take a different seed!\n",
        "* Average the predictions over three trees and inspect the result."
      ],
      "metadata": {
        "id": "xBafIdqiaPg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here"
      ],
      "metadata": {
        "id": "ZbJx1F7HPEy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Regression <a name=\"two-two\"></a>\n",
        "A `scikit-learn` bagging model for regression is implemented in the `sklearn.ensemble.BaggingRegressor` class: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor).\n",
        "\n",
        "*class sklearn.ensemble.BaggingRegressor(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0, base_estimator='deprecated')*\n",
        "\n",
        "Let's fit a bagged tree ensemble for our toy regression dataset:"
      ],
      "metadata": {
        "id": "vXSDl1OflfhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get X and y data\n",
        "X = dfr.x.to_numpy().reshape(-1,1)\n",
        "y = dfr.y.to_numpy()"
      ],
      "metadata": {
        "id": "4rSJGXbt4PFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the bagging model\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=25, min_samples_leaf=3), n_estimators=100, max_samples=0.8, bootstrap=True, oob_score=True, random_state=0)\n",
        "# fit the bagging model\n",
        "bag_reg.fit(X,y)"
      ],
      "metadata": {
        "id": "FVzq7Mjc3SjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now visualize the result from our bagged model:"
      ],
      "metadata": {
        "id": "Z1_t6XtY4iUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to plot data and predictions for the regression example\n",
        "def plot_reg(dfr, pred=None):\n",
        "  dfr['pred'] = pred\n",
        "  ggout = ggplot(dfr, aes(x = 'x')) + geom_point(aes(y = 'y'), alpha = 0.3) + geom_line(aes(y = 'm'), colour = 'darkgreen', size = 1.5)\n",
        "  if pred is not None:\n",
        "    ggout = ggout + geom_line(aes(y = 'pred'), colour = 'darkred', size = 1.5)\n",
        "  return(ggout)"
      ],
      "metadata": {
        "id": "fVeU0hw6DE2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot prediction result\n",
        "plot_reg(dfr, pred = bag_reg.predict(X))"
      ],
      "metadata": {
        "id": "kknG6aiW4mmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Experiment with the parameter settings of the bagging ensemble and base learner to obtain a smooth fit."
      ],
      "metadata": {
        "id": "f6QIgf4hCzYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here"
      ],
      "metadata": {
        "id": "rYnyhETRCy4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Out-of-bag <a name=\"two-three\"></a>\n",
        "Models where we take bootsrap samples come with a nice feature, namely the **o**ut-**o**f-**b**ag (OOB) prediction and error: \n",
        "1. The OOB prediction for an observation is calculated by taking only those models where this instance was not represented in the bootstrap training sample, and averaging these predictions. \n",
        "1. The OOB error is then calculated by comparing the OOB prediction for each observation to the true value and this serves as a generalization measure like a cross-validation error. The big advantage of OOB is that it comes for free without the need for extra model fits, like for example in 5-fold CV. \n",
        "\n",
        "Both the predictions and score are returned as an attribute of a fitted model if `oob_score = True`:"
      ],
      "metadata": {
        "id": "wiprOB_T6RtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OOB predictions\n",
        "bag_reg.oob_prediction_"
      ],
      "metadata": {
        "id": "SIUR7odS7HEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OOB score\n",
        "bag_reg.oob_score_"
      ],
      "metadata": {
        "id": "fV8OGv4sREmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that this is the $R^2$ metric:"
      ],
      "metadata": {
        "id": "WzFum0l6-_ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# R2 score\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y, bag_reg.oob_prediction_)"
      ],
      "metadata": {
        "id": "MxKFiyop7zkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also calculate the OOB MSE in one of the following two ways:"
      ],
      "metadata": {
        "id": "nY5H9m1I_FY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE DIY\n",
        "np.mean((bag_reg.oob_prediction_ - y)**2)"
      ],
      "metadata": {
        "id": "szathlKW5Uzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE sklearn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y, bag_reg.oob_prediction_)"
      ],
      "metadata": {
        "id": "FP2Rk0l48AME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate the OOB MSE for different number of estimators (trees) in the ensemble:"
      ],
      "metadata": {
        "id": "16tgBbPCuZGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a vector for the number of estimators and oob score\n",
        "num_est = [5,10,20,50,75,100,200,400,800]\n",
        "oob_mse = np.zeros(len(num_est))\n",
        "# iterate over the list\n",
        "for i in range(len(num_est)):\n",
        "  print(num_est[i])\n",
        "  # fit a bagged model\n",
        "  bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=25, min_samples_leaf=3), n_estimators=num_est[i], max_samples=0.8, bootstrap=True, oob_score=True, random_state=0).fit(X,y)\n",
        "  # evaluate the OOB MSE\n",
        "  oob_mse[i] = mean_squared_error(y, bag_reg.oob_prediction_)\n",
        "# Collect ans inspect the results\n",
        "pd_oob = pd.DataFrame.from_dict({'num_est':num_est,'oob_mse':oob_mse})\n",
        "pd_oob"
      ],
      "metadata": {
        "id": "lG7EpuMBSJSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember the nice thing about this OOB error: it is a generalization metric that comes for free during model fitting. Next we will see how you can perform grid search cross-validation on a bagged regression model."
      ],
      "metadata": {
        "id": "bRdCQUMou6fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Grid search CV <a name=\"two-four\"></a>\n",
        "The exercise from the Section 2.2 showed us that it is possible to obtain a good model fit by manually tweaking some parameters. However, there are two very big drawbacks with that approach:\n",
        "1. This manual tweaking is time-consuming work and not fun to do.\n",
        "1. Validation of good happened on a visual basis but not in a quantitative way.\n",
        "\n",
        "A parameter grid search via cross-validation mediates both issues, giving an automatic way to try different settings and returning a quantifiable loss metric to base decisions on regarding what a \"good\" fit is. In `scikit-learn`, grid search CV is implemented in the `class sklearn.model_selection.GridSearchCV`: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
      ],
      "metadata": {
        "id": "qh-kfTyq7R6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a prameter grid and perform 5-fold CV to find the optimal values in the grid as follows:"
      ],
      "metadata": {
        "id": "IT_OiYif906O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the feature matrix and target vector\n",
        "Xs, ys = shuffle(X,y, random_state= 0)\n",
        "# define a parameter grid as a dict\n",
        "param_grid = {\n",
        "'estimator__max_depth' : [5, 10, 20],\n",
        "'n_estimators' : [20, 50, 100]\n",
        "}\n",
        "# initialize the model\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(min_samples_leaf=3), max_samples=0.8, bootstrap=True, oob_score=True, random_state=0)\n",
        "# initialize the 5-fold CV\n",
        "bag_reg_cv = GridSearchCV(bag_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "# fit the CV\n",
        "bag_reg_cv.fit(Xs,ys)"
      ],
      "metadata": {
        "id": "wJj8GTjlmHLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can collect the results from the `cv_results_` attribute:"
      ],
      "metadata": {
        "id": "lUplX8-x_TE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect results\n",
        "bag_reg_cv.cv_results_"
      ],
      "metadata": {
        "id": "qL_k0PDAHW5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect results\n",
        "results_cv = bag_reg_cv.cv_results_\n",
        "# store in a dataframe\n",
        "results_pd = pd.DataFrame.from_dict(\n",
        "    {'depth':results_cv['param_estimator__max_depth'].data,\n",
        "     'estimators':results_cv['param_n_estimators'].data,\n",
        "     'score':-results_cv['mean_test_score'],\n",
        "     'rank':results_cv['rank_test_score']}).sort_values('rank')\n",
        "# show the top results\n",
        "results_pd.iloc[0:6]"
      ],
      "metadata": {
        "id": "RkZwYvKqn_o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now plot the predictions for the optimal values according to our grid search:"
      ],
      "metadata": {
        "id": "RFBj_KdRDAqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the optimal alpha from the CV results\n",
        "opt_depth = results_pd[results_pd['rank'] == 1]['depth'].iloc[0]\n",
        "opt_estim = results_pd[results_pd['rank'] == 1]['estimators'].iloc[0]\n",
        "# calculate the predictions for this alpha value\n",
        "pred = BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=opt_depth, min_samples_leaf=3), n_estimators=opt_estim, max_samples=0.8, bootstrap=True, oob_score=True, random_state=0).fit(X,y).predict(X)\n",
        "# plot the predictions\n",
        "plot_reg(dfr,pred)"
      ],
      "metadata": {
        "id": "lxrhGXSkrmxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Expand the current grid search for the regression example to find an even better fit.\n",
        "* Experiment with the classification toy example of session 1 to fit a bagged classification model."
      ],
      "metadata": {
        "id": "YsUXzfE3JPY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data generation classification example\n",
        "np.random.seed(54321)\n",
        "x1 = np.repeat(np.arange(0.1, 10.1, 0.1), 100)\n",
        "x2 = np.tile(np.arange(0.1, 10.1, 0.1), 100)\n",
        "X_clf = np.stack([x1,x2], axis=1)\n",
        "y_clf = np.zeros(len(x1), dtype=int)\n",
        "y_clf += (x1 + 2*x2 < 8).astype(int)\n",
        "y_clf += (3*x1 + x2 > 30).astype(int)\n",
        "y_clf += np.round(np.random.normal(loc=0, scale=0.3, size=len(y_clf))).astype(int)\n",
        "y_clf = np.clip(y_clf, 0, 1)\n",
        "dfc = pd.DataFrame.from_dict({'x1':x1,'x2':x2,'y':y_clf})\n",
        "dfc['y'] = dfc['y'].astype(\"category\")"
      ],
      "metadata": {
        "id": "PasQT3wScQoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here"
      ],
      "metadata": {
        "id": "MG-VCWefJcFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3 - Random forest <a name=\"three\"></a>\n",
        "A `scikit-learn` random forest regressor is implemented in the `sklearn.ensemble.RandomForestRegressor`: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor).\n",
        "\n",
        "*class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)*"
      ],
      "metadata": {
        "id": "5Ps2sRk2DWS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Dominant features <a name=\"three-one\"></a>\n",
        "We start the introduction of random forests by showing the need for random feature sampling in the case of dominant features within the MTPL dataset. Let's read and pre-process the data:"
      ],
      "metadata": {
        "id": "p0-2twIrS42b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the MTPL data\n",
        "mtpl = pd.read_csv(\"https://katrienantonio.github.io/hands-on-machine-learning-R-module-1/data/PC_data.txt\", delimiter = \"\\t\", usecols=list(range(1,14)))\n",
        "# transform the column names to lowercase\n",
        "mtpl.columns = mtpl.columns.str.lower()\n",
        "# rename the exp column to expo\n",
        "mtpl = mtpl.rename(columns= {'exp': 'expo'})\n",
        "# map string values to integers for certain columns\n",
        "mtpl['coverage'] = mtpl['coverage'].map({'TPL':0, 'PO':1, 'FO':2})\n",
        "mtpl['fleet'] = mtpl['fleet'].map({'N':0, 'Y':1})\n",
        "mtpl['fuel'] = mtpl['fuel'].map({'gasoline':0, 'diesel':1})\n",
        "mtpl['use'] = mtpl['use'].map({'private':0, 'work':1})\n",
        "mtpl['sex'] = mtpl['sex'].map({'male':0, 'female':1})\n",
        "# print the shape\n",
        "print(mtpl.shape)\n",
        "# show the first observations\n",
        "mtpl.head(100)"
      ],
      "metadata": {
        "id": "1mnjdFtGiDQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create data objects for the features `X`, the target `y` and the weights `w`:"
      ],
      "metadata": {
        "id": "tsoVXU0hTFtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cols to retain as features\n",
        "feat_cols = ['bm','ageph','agec','power','coverage','fuel','sex','fleet','use']\n",
        "# subset the data\n",
        "X_mtpl_freq = mtpl[feat_cols]\n",
        "# print the shape\n",
        "print(X_mtpl_freq.shape)\n",
        "# show the features\n",
        "X_mtpl_freq\n",
        "\n",
        "# claim frequency (nclaims/expo) as target\n",
        "y_mtpl_freq = np.array(mtpl.nclaims/mtpl.expo)\n",
        "# exposure as weights\n",
        "w_mtpl_freq = np.array(mtpl.expo)"
      ],
      "metadata": {
        "id": "t_Qe7aHUqkGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create two arrays which contain random bootstrap sample ids for the MTPL data:"
      ],
      "metadata": {
        "id": "B--Cwy-EjdhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nrow_mtpl = mtpl.shape[0]\n",
        "# get a bootstrap sample with replacement\n",
        "random.seed(1234)\n",
        "id1 = random.choices(range(nrow_mtpl), k=int(0.8*nrow_mtpl))\n",
        "# get a bootstrap sample with replacement\n",
        "random.seed(5678)\n",
        "id2 = random.choices(range(nrow_mtpl), k=int(0.8*nrow_mtpl))"
      ],
      "metadata": {
        "id": "ObeYwkfpjd2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we fit a decision tree to each subset of the MTPL data:"
      ],
      "metadata": {
        "id": "LBimAehWkIPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and fit the tree to bootstrap sample 1\n",
        "tree_freq1 = DecisionTreeRegressor(criterion='poisson', max_depth=2, min_samples_split=10000, min_samples_leaf=5000)\n",
        "tree_freq1.fit(X=X_mtpl_freq.iloc[id1], y=y_mtpl_freq[id1], sample_weight=w_mtpl_freq[id1])\n",
        "# plot the tree\n",
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plot_tree(tree_freq1, feature_names=X_mtpl_freq.columns);"
      ],
      "metadata": {
        "id": "LIsvkBZOkIqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and fit the tree to bootstrap sample 2\n",
        "tree_freq2 = DecisionTreeRegressor(criterion='poisson', max_depth=2, min_samples_split=10000, min_samples_leaf=5000)\n",
        "tree_freq2.fit(X=X_mtpl_freq.iloc[id2], y=y_mtpl_freq[id2], sample_weight=w_mtpl_freq[id2])\n",
        "# plot the tree\n",
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plot_tree(tree_freq2, feature_names=X_mtpl_freq.columns);"
      ],
      "metadata": {
        "id": "ps0RfV3Gkn7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how similar both trees are due to the dominance of the bonus malus feature? Aggregating similar trees will not result in a great reduction of variance and will therefore limit the predictive performance of the ensemble."
      ],
      "metadata": {
        "id": "-HBaBPJLmJ1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Feature sampling <a name=\"three-two\"></a>\n",
        "We fit a random forest regressor to the MTPL data and randomly sample features via the `max_features` parameter:"
      ],
      "metadata": {
        "id": "W5cgIybong9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a random forest\n",
        "rf_freq = RandomForestRegressor(n_estimators=100, max_features = 0.3, criterion='poisson', max_depth=2, min_samples_split=10000, min_samples_leaf=5000, random_state=0)\n",
        "# fit the tree to our target with weights\n",
        "rf_freq.fit(X=X_mtpl_freq, y=y_mtpl_freq, sample_weight=w_mtpl_freq)\n",
        "# print the tree\n",
        "rf_freq"
      ],
      "metadata": {
        "id": "8vGC_PoAc0ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take two random trees from the ensemble and inspect the first splits and which features are chosen:"
      ],
      "metadata": {
        "id": "gkJ_ub2bo75N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot a tree from the ensemble\n",
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plot_tree(rf_freq.estimators_[20], feature_names=X_mtpl_freq.columns);"
      ],
      "metadata": {
        "id": "5tg_ECIYocGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot a tree from the ensemble\n",
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plot_tree(rf_freq.estimators_[90], feature_names=X_mtpl_freq.columns);"
      ],
      "metadata": {
        "id": "PY4-5O_Fo0J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the random feature sampling at each split we now have individual trees that are more decorrelated to improve variance reduction, hence a random forest."
      ],
      "metadata": {
        "id": "GY67NAQ-UHtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Tuning & insights <a name=\"three-three\"></a>\n",
        "We now perform a simple grid search to find a good claim frequency random forest and use interpretation tools to gain insights from our model."
      ],
      "metadata": {
        "id": "87pyKSLrl3w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a parameter grid as a dict\n",
        "param_grid = {\n",
        "'n_estimators' : [50, 100],\n",
        "'max_features' : [0.6, 1],\n",
        "'max_depth' : [5, 10]\n",
        "}\n",
        "# initialize the model and CV\n",
        "rf_freq = RandomForestRegressor(criterion='poisson', min_samples_split=10000, min_samples_leaf=2000, random_state=0)\n",
        "rf_freq_cv = GridSearchCV(rf_freq, param_grid, cv=5, scoring='neg_mean_poisson_deviance')\n",
        "# fit the CV\n",
        "rf_freq_cv.fit(X_mtpl_freq, y=y_mtpl_freq, sample_weight=w_mtpl_freq)"
      ],
      "metadata": {
        "id": "0idfqKB1iuIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the CV results:"
      ],
      "metadata": {
        "id": "h0srZm3jvMUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect results\n",
        "results_cv = rf_freq_cv.cv_results_\n",
        "# store in a dataframe\n",
        "results_pd = pd.DataFrame.from_dict(\n",
        "    {'depth':results_cv['param_max_depth'].data,\n",
        "     'estimators':results_cv['param_n_estimators'].data,\n",
        "     'features' : results_cv['param_max_features'].data,\n",
        "     'score':-results_cv['mean_test_score'],\n",
        "     'rank':results_cv['rank_test_score']}).sort_values('rank')\n",
        "# show the top results\n",
        "results_pd.iloc[0:6]"
      ],
      "metadata": {
        "id": "grCw_zfbjU_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the best estimator from the CV results:"
      ],
      "metadata": {
        "id": "vSS_5WO7vOQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the best model from the CV results\n",
        "rf_freq_opt = rf_freq_cv.best_estimator_"
      ],
      "metadata": {
        "id": "B7955FlquU-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the feature importance metric:"
      ],
      "metadata": {
        "id": "HzuXa6V0vSkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the feature names and importance scores\n",
        "rf_freq_fi = pd.DataFrame({'feature':rf_freq_opt.feature_names_in_, 'importance':rf_freq_opt.feature_importances_}).sort_values('importance', ascending=False)\n",
        "# inspect the results\n",
        "rf_freq_fi"
      ],
      "metadata": {
        "id": "-jCwUVCxuEFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show some the PDP for all features in the MTPL data:"
      ],
      "metadata": {
        "id": "6zh2pGLsvY6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create pdps for a couple of features\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "PartialDependenceDisplay.from_estimator(rf_freq_opt, X_mtpl_freq, features = ['bm','ageph','power','fuel','agec','coverage'], categorical_features=['fuel','coverage'], kind='average', ax=ax);"
      ],
      "metadata": {
        "id": "7IuQmGlUunTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you think of these effects compared to those from the decision trees?"
      ],
      "metadata": {
        "id": "FkXtwX1qxE35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Experiment with the grid search to find a better performing model. Beware of the tuning time needed.\n",
        "* How do those PDPs look?"
      ],
      "metadata": {
        "id": "S1tXFXVrrgiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here"
      ],
      "metadata": {
        "id": "W1tofEfysC_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4 - Boosting <a name=\"four\"></a>\n",
        "A `scikit-learn` gradient boosting regressor is implemented in the `sklearn.ensemble.GradientBoostingRegressor`: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor).\n",
        "\n",
        "*class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)*"
      ],
      "metadata": {
        "id": "fTJ5er4oDvA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Parameters <a name=\"four-one\"></a>\n",
        "Let's inspect how certain parameters affect the boosting process."
      ],
      "metadata": {
        "id": "1Et7hjwhK35p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the boosting model\n",
        "bst_reg = GradientBoostingRegressor(loss='squared_error', learning_rate=1, n_estimators=5, max_depth=1, min_samples_leaf=5, subsample=0.8, random_state=0)\n",
        "# fit the boosting model\n",
        "bst_reg.fit(X,y)\n",
        "# plot prediction result\n",
        "plot_reg(dfr, pred = bst_reg.predict(X))"
      ],
      "metadata": {
        "id": "TPLye6c0k5Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we lower the learning rate?"
      ],
      "metadata": {
        "id": "65F4acR3OpOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the boosting model\n",
        "bst_reg = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=5, max_depth=1, min_samples_leaf=5, subsample=0.8, random_state=0)\n",
        "# fit the boosting model\n",
        "bst_reg.fit(X,y)\n",
        "# plot prediction result\n",
        "plot_reg(dfr, pred = bst_reg.predict(X))"
      ],
      "metadata": {
        "id": "pY1bnO7Jnp2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we increase the individual tree depth?"
      ],
      "metadata": {
        "id": "TbpxwnltO7yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the boosting model\n",
        "bst_reg = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=5, max_depth=2, min_samples_leaf=5, subsample=0.8, random_state=0)\n",
        "# fit the boosting model\n",
        "bst_reg.fit(X,y)\n",
        "# plot prediction result\n",
        "plot_reg(dfr, pred = bst_reg.predict(X))"
      ],
      "metadata": {
        "id": "7y6wdtqSnpz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we increase the number of boosting iterations?"
      ],
      "metadata": {
        "id": "zIGH1gfTRMYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the boosting model\n",
        "bst_reg = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=50, max_depth=2, min_samples_leaf=5, subsample=0.8, random_state=0)\n",
        "# fit the boosting model\n",
        "bst_reg.fit(X,y)\n",
        "# plot prediction result\n",
        "plot_reg(dfr, pred = bst_reg.predict(X))"
      ],
      "metadata": {
        "id": "aIKxvfmDnpq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the iteration process over time:"
      ],
      "metadata": {
        "id": "9cnZNDOfpGCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a plot object\n",
        "plt.figure(figsize=(20, 20),dpi=100)\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.suptitle(\"Boosting predictions for different iterations\", fontsize=18, y=0.95)\n",
        "# iterate over the tree list and take every 5th item\n",
        "for i, indx in enumerate(list(range(1, 90, 10))): \n",
        "  # fit and make a prediction for this iteration\n",
        "  pred = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=indx, max_depth=2, min_samples_leaf=5, subsample=0.8, random_state=0).fit(X,y).predict(X)\n",
        "  # plot the predictions in a subplot\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  ax.scatter(x,y, color='gray', s=2)\n",
        "  ax.plot(x,m,color='green')\n",
        "  ax.plot(x,pred,color='red')\n",
        "  ax.set_title(f'iteration: {indx}')\n",
        "  ax.set_xlabel('x')\n",
        "  ax.set_ylabel('y')"
      ],
      "metadata": {
        "id": "cDACX1-QpGd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "Adjust the `learning_rate` parameter in the cell above to the following values and explain what you see:\n",
        "- `learning_rate` = 1\n",
        "- `learning_rate` = 0.01\n",
        "\n",
        "Is this what you would expect?"
      ],
      "metadata": {
        "id": "NJN7acGwq8GT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Out-of-bag <a name=\"four-two\"></a>\n",
        "By setting `subsample` < 1, we are performing stochastic boosting where each individual tree base learner is fit on a sample of the original dataset. This means that, just like with bagging, we have observations that are not being used by certain base learners are are therefore out-of-bag."
      ],
      "metadata": {
        "id": "RVuMo6dPU3Mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's retake the boosting regressor from before but with more boosting iterations:"
      ],
      "metadata": {
        "id": "2ViSLZtoVO31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the boosting model\n",
        "bst_reg = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=100, max_depth=2, min_samples_leaf=5, subsample=0.8, random_state=0)\n",
        "# fit the boosting model\n",
        "bst_reg.fit(X,y)"
      ],
      "metadata": {
        "id": "evlnRFp_nxwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The improvement in the OOB error per boosting iteration is saved as the `ob_improvement_` attribute of a fitted boosting model: "
      ],
      "metadata": {
        "id": "xX40FQMBYwU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get oob improvement\n",
        "bst_reg.oob_improvement_"
      ],
      "metadata": {
        "id": "ML2dHIucnxu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the OOB error improvement over time as follows:"
      ],
      "metadata": {
        "id": "WyNJV7ls0vtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the oob improvement in a dataframe\n",
        "bst_oob = pd.DataFrame.from_dict({'iteration':range(1,bst_reg.n_estimators_+1),'oob_improv':bst_reg.oob_improvement_})\n",
        "# plot the evolution of OOB improvement over time\n",
        "ggplot(bst_oob, aes(x='iteration', y='oob_improv')) + geom_line()"
      ],
      "metadata": {
        "id": "08QC3tMcnLfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the improvement becomes negligible and even negative (indicating worse performance) after a certain number of iterations. This can point towards overfitting as the OOB error serves as a generalization error metric. The number of boosting iterations is of course a tuning parameter, but we can also make use of another interesting trick."
      ],
      "metadata": {
        "id": "FMFypEec1d6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Early stopping <a name=\"four-three\"></a>\n",
        "Early stopping is a technique to stop boosting iterations before the requested number of trees `n_estimators` is reached. This is done by tracking performance over time for a specific validation set and can be implemented by setting the following parameters:\n",
        "- `n_iter_no_change`: early stop if the loss does not improve for this many iterations (default `None` to disable early stopping).\n",
        "- `validation_fraction`: proportion of training data to use for early stopping loss (default = 0.1).\n",
        "- `tol`: tolerance for the early stopping loss (default = 1e-4). \n",
        "\n",
        "When the loss on the validation set is not improving by at least `tol` for `n_iter_no_change` iterations, the training stops."
      ],
      "metadata": {
        "id": "1u3gZBfhAa4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's retake the previous example again, but now with early stopping enabled:"
      ],
      "metadata": {
        "id": "KUfya-1RCXWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the boosting model\n",
        "bst_reg = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=100, max_depth=2, min_samples_leaf=5, subsample=0.8, random_state=0, n_iter_no_change = 5)\n",
        "# fit the boosting model\n",
        "bst_reg.fit(X,y)"
      ],
      "metadata": {
        "id": "umxtmCtMBUr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We requested 100 boosting iterations, but we ended up with only:"
      ],
      "metadata": {
        "id": "wuphrto-DojC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of estimators\n",
        "bst_reg.n_estimators_"
      ],
      "metadata": {
        "id": "0ae08Ws4BUpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add indicators for the early stopping round and the tolerance to the evolution of the OOB error improvement:"
      ],
      "metadata": {
        "id": "XPL4QA8ZEQpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot early stop together with OOB\n",
        "ggplot(bst_oob.query('iteration > 10'), aes(x='iteration', y='oob_improv')) + geom_line() + geom_vline(xintercept=bst_reg.n_estimators_) + geom_hline(yintercept=1e-4)"
      ],
      "metadata": {
        "id": "gpFT7bKtEPaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping is calculated on a validation set and the OOB error improvement on the out-of-bag samples, but both can lead to very similar conclusions about the optimal number of boosting iterations."
      ],
      "metadata": {
        "id": "iOsaNjEUVopN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Other implementations <a name=\"four-four\"></a>\n",
        "`GradientBoostingRegressor`, the standard boosting implementation of `sklearn`, has three big disadvantages:\n",
        "- slow for large datasets: no parallel computing and exhaustive algorithms\n",
        "- limited number of options: no regularization and support for missing values for example\n",
        "- limited number of loss functions: squared_error, absolute_error, huber, quantile\n",
        "\n",
        "There are plenty of other boosting implementations available for more advanced modeling:\n",
        "- `sklearn.ensemble.HistGradientBoostingRegressor`: [docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor)\n",
        "- `lightgbm.LGBMRegressor`: [docs](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor)\n",
        "- `catboost.CatBoostRegressor`: [docs](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor)\n",
        "- `xgboost.XGBRegressor`: [docs](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn)\n",
        "\n",
        "We will be going into more detail about `xgboost` via the `scikit-learn` wrapper interface for XGBoost."
      ],
      "metadata": {
        "id": "E7GhJPEG1JPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5 - XGBoost <a name=\"five\"></a>\n",
        "One of the most popular packages in python (and beyond) for stochastic gradient boosting machines is `xgboost`. This library provides an efficient implementation of the gradient boosting algorithm with some useful extra features.\n",
        "\n",
        "The function `class xgboost.XGBRegressor(*, objective='reg:squarederror', **kwargs)` presents, among others, the following parameters:\n",
        "* `n_estimators`: number of gradient boosted trees; this is equivalent to the number of boosting rounds\n",
        "* `max_depth`: max. tree depth for base learners\n",
        "* `learning_rate`: boosting learning rate\n",
        "* `objective`: objective function to be used\n",
        "* `tree_method`: which tree method to use, default = `auto`\n",
        "* `reg_alpha`: L1 regularization term on weights\n",
        "* `reg_lambda`: L2 regularization term on weights\n",
        "* `monotone_constraints`: Constraint of variable monotonicity\n",
        "* ...\n",
        "\n",
        "A list of all available parameters can be found [here](https://xgboost.readthedocs.io/en/stable/parameter.html).\n",
        "\n",
        "Let's build claim frequency and severity models for our MTPL dataset with XGBoost."
      ],
      "metadata": {
        "id": "0bZ0D2MdJu2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print mtpl data\n",
        "mtpl"
      ],
      "metadata": {
        "id": "ChMKSPKK7ke7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Claim frequency <a name=\"five-one\"></a>\n",
        "We first calculate the required targets and weights for our claim frequency regression problem like before:"
      ],
      "metadata": {
        "id": "oormmPwpKcMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cols to retain as features\n",
        "feat_cols = ['bm','ageph','agec','power','coverage','fuel','sex','fleet','use']\n",
        "# subset the data\n",
        "X_mtpl_freq = mtpl[feat_cols]\n",
        "# print the shape\n",
        "print(X_mtpl_freq.shape)\n",
        "\n",
        "# claim frequency (nclaims/expo) as target\n",
        "y_mtpl_freq = np.array(mtpl.nclaims/mtpl.expo)\n",
        "# exposure as weights\n",
        "w_mtpl_freq = np.array(mtpl.expo)"
      ],
      "metadata": {
        "id": "MHALt4eUJQl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can fit an XGBoost model for claim frequency via the `count:poisson` objective:\n"
      ],
      "metadata": {
        "id": "Ud_85TBpclK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the xgboost model\n",
        "xgb_freq = xgb.XGBRegressor(n_estimators = 500,\n",
        "                            objective='count:poisson',\n",
        "                            monotone_constraints = (1,0,0,0,0,0,0,0,0),\n",
        "                            max_depth = 3,\n",
        "                            learning_rate = 0.01,\n",
        "                            base_score = np.sum(y_mtpl_freq * w_mtpl_freq)/np.sum(w_mtpl_freq))\n",
        "# fit the xgboost model\n",
        "xgb_freq.fit(X_mtpl_freq, y_mtpl_freq, sample_weight=w_mtpl_freq)"
      ],
      "metadata": {
        "id": "vEq4tTQ0Kg4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make predictions from this model on the target scale via the `predict` method:\n"
      ],
      "metadata": {
        "id": "Pe0z8MWNdSEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on the target scale\n",
        "pred_on_target_scale = np.round(xgb_freq.predict(X_mtpl_freq), 5)\n",
        "plt.hist(pred_on_target_scale, bins=10);"
      ],
      "metadata": {
        "id": "hftr7fZRKhdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain annual claim frequency predictions, without exposure being taken into account:"
      ],
      "metadata": {
        "id": "yLPdjW1U_0xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summary statistics of the predictions\n",
        "pd.Series(pred_on_target_scale).describe()"
      ],
      "metadata": {
        "id": "D_3rT2Fw-33s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make predictions on the log scale by setting `output_margin = True`:"
      ],
      "metadata": {
        "id": "X0wezyP8kfMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on raw log scale\n",
        "pred_on_log_scale = np.round(xgb_freq.predict(X_mtpl_freq, output_margin = True), 5)\n",
        "plt.hist(pred_on_log_scale, bins=10);"
      ],
      "metadata": {
        "id": "SX7I6zGZKhZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make predictions from a subset of the ensemble via the `iteration_range` parameter:"
      ],
      "metadata": {
        "id": "3uBhKhS4lATe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on subset\n",
        "pred_on_subset = np.round(xgb_freq.predict(X_mtpl_freq, iteration_range = (0,5)), 5)\n",
        "plt.hist(pred_on_subset, bins=10);"
      ],
      "metadata": {
        "id": "WzSqU-hEAHS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Claim severity <a name=\"five-two\"></a>\n",
        "We first calculate the required targets and weights for our claim severity regression problem like before:"
      ],
      "metadata": {
        "id": "ICU2bLWFKhx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subset the data\n",
        "mtpl_sev = mtpl.query('amount > 1 & amount < 100000')\n",
        "X_mtpl_sev = mtpl_sev[feat_cols]\n",
        "# print the shape\n",
        "print(X_mtpl_sev.shape)\n",
        "\n",
        "# claim severity (amount/nclaims) as target\n",
        "y_mtpl_sev = np.array(mtpl_sev.avg)\n",
        "# number of claims as weights\n",
        "w_mtpl_sev = np.array(mtpl_sev.nclaims)"
      ],
      "metadata": {
        "id": "xfbMkE_gbAqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can fit an XGBoost model for claim frequency via the `reg:gamma` objective:\n"
      ],
      "metadata": {
        "id": "-WZ3IudkGslL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the xgboost model\n",
        "xgb_sev = xgb.XGBRegressor(n_estimators = 100,\n",
        "                           objective='reg:gamma',\n",
        "                           max_depth = 3,\n",
        "                           learning_rate = 0.01,\n",
        "                           base_score = np.sum(y_mtpl_sev * w_mtpl_sev)/np.sum(w_mtpl_sev))\n",
        "# fit the xgboost model\n",
        "xgb_sev.fit(X_mtpl_sev, y_mtpl_sev, sample_weight=w_mtpl_sev)"
      ],
      "metadata": {
        "id": "IxuJuPRWGTsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make predictions from this model on the target scale via the `predict` method:"
      ],
      "metadata": {
        "id": "W_-BFBXc6OLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on the target scale\n",
        "pred_on_target_scale = np.round(xgb_sev.predict(X_mtpl_freq), 2)\n",
        "plt.hist(pred_on_target_scale, bins=10);"
      ],
      "metadata": {
        "id": "x_y2SdKnKkx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that we are no longer underestimating the mean of our severity distribution like we were doing with the decision trees on the log-transformed severity:"
      ],
      "metadata": {
        "id": "rdUgf0sYC6RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean of the predictions\n",
        "pred_on_target_scale.mean()"
      ],
      "metadata": {
        "id": "-8rk9-25HUD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean of the portfolio severity\n",
        "mtpl_sev.avg.mean()"
      ],
      "metadata": {
        "id": "Fnrnc0KbHecz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Random search CV <a name=\"five-three\"></a>\n",
        "A grid search has the advantage that all possible combinations of tuning parameters are considered and the optimal combination is found. This procedure however becomes extremely time-consuming if a lot of tuning parameters are involved. In such a situation, a randomized search is better to save computation time. A randomized search simply tries *m* possible combinations out of *n* cases and returns the best performing one from this subset.\n",
        "\n",
        "We start by defining our possible parameter values and initialize an `XGBRegressor` for claim frequency:"
      ],
      "metadata": {
        "id": "qkveaUjRDnSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define dictionary for search\n",
        "param_dict = {'max_depth' : [1, 3, 5, 7, 9],\n",
        "              'n_estimators' : [100, 200, 300],\n",
        "              'colsample_bynode' : [0.5, 0.75, 1],\n",
        "              'lambda' : [0, 0.1, 1],\n",
        "              'alpha' : [0, 0.1, 1]}\n",
        "\n",
        "# Initialize an XGBRegressor\n",
        "xgb_init = xgb.XGBRegressor(booster='gbtree',\n",
        "                            learning_rate = 0.01,\n",
        "                            objective='count:poisson',\n",
        "                            eval_metric = 'poisson-nloglik',\n",
        "                            monotone_constraints = (1,0,0,0,0,0,0,0,0),\n",
        "                            base_score = np.sum(y_mtpl_freq * w_mtpl_freq)/np.sum(w_mtpl_freq))"
      ],
      "metadata": {
        "id": "gIeDmDJ8ECr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `sklearn.model_selection.RandomizedSearchCV` presents the following additional (or other) parameters:\n",
        "\n",
        "* `param_distributions`: the dictionary object that holds the distribution of possible hyperparameters you want to try\n",
        "* `n_iter`: the number of random tuning parameter combinations to try out\n",
        "* `random_state`: an initializer to make the random selection process repeatable."
      ],
      "metadata": {
        "id": "ZrQMnTxFGb7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform cross_validation\n",
        "xgb_randomsearch = RandomizedSearchCV(estimator=xgb_init,\n",
        "                                      param_distributions=param_dict,\n",
        "                                      scoring='neg_mean_poisson_deviance',\n",
        "                                      n_iter=2,\n",
        "                                      verbose=1,\n",
        "                                      cv=5,\n",
        "                                      random_state = 54321)\n",
        "xgb_randomsearch.fit(X_mtpl_freq, y_mtpl_freq, sample_weight=w_mtpl_freq)"
      ],
      "metadata": {
        "id": "q0k6x8YYF6cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can collect the random search results from the `cv_results_` attribute:"
      ],
      "metadata": {
        "id": "JoUZ26NLK6tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get cv results\n",
        "pd.DataFrame(xgb_randomsearch.cv_results_).sort_values('rank_test_score')"
      ],
      "metadata": {
        "id": "MoHmeAW7HEKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the best parameter combination from the `best_params_` attribute:"
      ],
      "metadata": {
        "id": "mUwNGoA_LIdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get best combination\n",
        "xgb_randomsearch.best_params_"
      ],
      "metadata": {
        "id": "w5Tjftk6IE1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the best model from the `best_estimator_` attribute:"
      ],
      "metadata": {
        "id": "KKzmivquLRY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get best model\n",
        "xgb_best = xgb_randomsearch.best_estimator_\n",
        "xgb_best"
      ],
      "metadata": {
        "id": "H3JYmLWeIEs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the feature importance scores from the `feature_importances_` attribute:"
      ],
      "metadata": {
        "id": "5VqL7FL0Lafp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get feature importance\n",
        "pd.DataFrame({'feature':xgb_best.feature_names_in_, 'importance':xgb_best.feature_importances_}).sort_values('importance', ascending=False)"
      ],
      "metadata": {
        "id": "XuEJBWJ4If30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize some PDP effects via the `PartialDependenceDisplay.from_estimator` function:"
      ],
      "metadata": {
        "id": "eTtZCSrPLnCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize pdps\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "ax.set_title(\"PDPs\")\n",
        "PartialDependenceDisplay.from_estimator(xgb_best, X=X_mtpl_freq[0:1000], features = ['bm','ageph',('ageph','power')], ax=ax);"
      ],
      "metadata": {
        "id": "wJ2Iqr-YIy_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "Time to put everything together and find the best possible claim frequency model:\n",
        "\n",
        "* Choose 3 model classes, for example: a regression tree, a random forest and an xgboost model.\n",
        "* Split the MTPL data in train and test data.\n",
        "* Find the optimal tuning parameters for each model class by performing x-fold cross-validation via a grid or random search on the train data.\n",
        "* Test the performance of each model on the out-of-sample test data.\n",
        "* Which one is the winner?"
      ],
      "metadata": {
        "id": "GL6G_dsq1_Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#add your code here"
      ],
      "metadata": {
        "id": "e2EX6kDF2y51"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}