{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> <span style=\"color:black\"> Hands-on Machine Learning with Python  </h1> </center> \n",
        "<center> <h2> <span style=\"color:red\"> Module 2: Tree-based machine learning methods </h1> </center>\n",
        "<center> <h3> <span style=\"color:red\"> Session 1: Decision trees </h1> </center>"
      ],
      "metadata": {
        "id": "UWk6ioM5BZKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structure of the notebook\n",
        "\n",
        "* [Chapter 1 - Introduction](#one)\n",
        "    + [1.1 Objectives of the notebook](#one-one)\n",
        "    + [1.2 Library requirements](#one-two)\n",
        "\n",
        "* [Chapter 2 - Regression tree](#two)\n",
        "    + [2.1 Toy example](#two-one)\n",
        "    + [2.2 Parameter settings](#two-two)\n",
        "    + [2.3 Grid search cross-validation](#two-three)\n",
        "        \n",
        "* [Chapter 3 - Classification tree](#three)\n",
        "    + [3.1 Toy example](#three-one)\n",
        "    + [3.2 Grid search cross-validation](#three-two)\n",
        "\n",
        "* [Chapter 4 - Actuarial tree](#four)\n",
        "    + [4.1 MTPL data](#four-one)\n",
        "    + [4.2 Claim frequency](#four-two)\n",
        "    + [4.3 Claim severity](#four-three)\n",
        "\n",
        "* [Chapter 5 - Interpretation tools](#five)\n",
        "    + [5.1 Feature importance](#five-one)\n",
        "    + [5.2 Partial dependence](#five-two)\n"
      ],
      "metadata": {
        "id": "rLwSpISOFdnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1 - Introduction <a name=\"one\"></a>"
      ],
      "metadata": {
        "id": "wkKSGnBcFdaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Objectives of the notebook <a name=\"one-one\"></a>\n",
        "The objectives of this notebook are to:\n",
        "1. Build decision trees for typical regression, classification and actuarial problems.\n",
        "1. Tune the parameters of decision trees to obtain optimal performance.\n",
        "1. Inspect decision trees to gain insights in the underlying decision process."
      ],
      "metadata": {
        "id": "NgXpTxuHFxre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Library requirements <a name=\"one-two\"></a>\n",
        "We start by importing all the required Python packages for this notebook."
      ],
      "metadata": {
        "id": "St-Z1wDnF5ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from plotnine import ggplot, geom_point, geom_line, aes, theme_set, theme_bw\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.inspection import permutation_importance, partial_dependence, PartialDependenceDisplay\n",
        "\n",
        "# set the black and white theme for ggplot to get rid of gray backgrounds\n",
        "theme_set(theme_bw())"
      ],
      "metadata": {
        "id": "iitbIysoCrcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2 - Regression tree <a name=\"two\"></a>\n",
        "A `scikit-learn` regression tree is implemented in the `sklearn.tree.DecisionTreeRegressor`: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)."
      ],
      "metadata": {
        "id": "dsR2TjadCJfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Toy example <a name=\"two-one\"></a>\n",
        "We start by fitting a simple regression tree to a toy example with simulated data. The goal is to understand the principles of how regression trees model the underlying data."
      ],
      "metadata": {
        "id": "R6hRMlE1JpTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We simulate data from a sinusoidal pattern with some normally distributed noise on top of it:"
      ],
      "metadata": {
        "id": "wyWCxSBRLBp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMFeHHbiBNG4"
      },
      "outputs": [],
      "source": [
        "# set a seed for reproducibility\n",
        "np.random.seed(5678)\n",
        "# generate a x array from 0 to 2*pi\n",
        "x = np.linspace(start=0, stop=2*math.pi, num=500)\n",
        "# generate the true model m as the sin of x\n",
        "m = np.sin(x)\n",
        "# generate the observed y by adding normal noise to m\n",
        "y = m + np.random.normal(loc=0, scale=0.5, size = len(m))\n",
        "# collect the arrays in a dataframe\n",
        "dfr = pd.DataFrame.from_dict({'x':x, 'm':m, 'y':y})\n",
        "# print the dataframe\n",
        "dfr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simulated data (gray points) and the underlying true model (green line) look as follows:"
      ],
      "metadata": {
        "id": "45ocnm6KMA4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot simulated data\n",
        "ggplot(dfr, aes(x = 'x')) + geom_point(aes(y = 'y'), alpha = 0.3) + geom_line(aes(y = 'm'), colour = 'darkgreen', size = 1.5)"
      ],
      "metadata": {
        "id": "nVJrD_DjMAdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before fitting our first model, we need to reshape the feature vector `x` to a feature matrix `X` because `sklearn` expects a 2D array:"
      ],
      "metadata": {
        "id": "-G_cimQ3RqFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the shape of x\n",
        "print(x.shape)\n",
        "# reshape x to a matrix\n",
        "X = x.reshape(-1, 1)\n",
        "# print the shape of X\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "7P3JjZNTRpvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a first try, we fit a decision stump (a tree with only one split and two leaf nodes) to our data:"
      ],
      "metadata": {
        "id": "x7Tcz1hmOmIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a DecisionTreeRegressor with max depth equal to 1\n",
        "tree_reg1 = DecisionTreeRegressor(criterion='squared_error', max_depth=1)\n",
        "# fit the tree to our data\n",
        "tree_reg1 = tree_reg1.fit(X, y)\n",
        "# print the tree object\n",
        "tree_reg1"
      ],
      "metadata": {
        "id": "0UuqcQabRM2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the tree object does not yield any information, so let's dig a little bit deeper. The `scikit-learn` page contains a guide on how to [understand the tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py) from `tree_` attributes like `children`, `feature` and `threshold`:"
      ],
      "metadata": {
        "id": "1N8hleLWTFtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to explore the tree structure\n",
        "def print_tree(tree_obj): \n",
        "  n_nodes = tree_obj.tree_.node_count\n",
        "  children_left = tree_obj.tree_.children_left\n",
        "  children_right = tree_obj.tree_.children_right\n",
        "  feature = tree_obj.tree_.feature\n",
        "  threshold = tree_obj.tree_.threshold\n",
        "\n",
        "  node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
        "  is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
        "  stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
        "  while len(stack) > 0:\n",
        "      # `pop` ensures each node is only visited once\n",
        "      node_id, depth = stack.pop()\n",
        "      node_depth[node_id] = depth\n",
        "\n",
        "      # If the left and right child of a node is not the same we have a split\n",
        "      # node\n",
        "      is_split_node = children_left[node_id] != children_right[node_id]\n",
        "      # If a split node, append left and right children and depth to `stack`\n",
        "      # so we can loop through them\n",
        "      if is_split_node:\n",
        "          stack.append((children_left[node_id], depth + 1))\n",
        "          stack.append((children_right[node_id], depth + 1))\n",
        "      else:\n",
        "          is_leaves[node_id] = True\n",
        "\n",
        "  print(\n",
        "      \"The binary tree structure has {n} nodes and has \"\n",
        "      \"the following tree structure:\\n\".format(n=n_nodes)\n",
        "  )\n",
        "  for i in range(n_nodes):\n",
        "      if is_leaves[i]:\n",
        "          print(\n",
        "              \"{space}node={node} is a leaf node.\".format(\n",
        "                  space=node_depth[i] * \"\\t\", node=i\n",
        "              )\n",
        "          )\n",
        "      else:\n",
        "          print(\n",
        "              \"{space}node={node} is a split node: \"\n",
        "              \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
        "              \"else to node {right}.\".format(\n",
        "                  space=node_depth[i] * \"\\t\",\n",
        "                  node=i,\n",
        "                  left=children_left[i],\n",
        "                  feature=feature[i],\n",
        "                  threshold=threshold[i],\n",
        "                  right=children_right[i],\n",
        "              )\n",
        "          )"
      ],
      "metadata": {
        "id": "mApatR8rThHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applied to our tree example the results looks as follows:"
      ],
      "metadata": {
        "id": "Ia46x8LGUsE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the tree structure\n",
        "print_tree(tree_reg1)"
      ],
      "metadata": {
        "id": "q4Q89juFUsPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the `sklearn.tree.plot_tree` function to visualize the tree structure, along with node counts, predictions and error metrics."
      ],
      "metadata": {
        "id": "xfvHCyDeXv0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the tree structure\n",
        "plt.figure(figsize=(5, 5), dpi=100)\n",
        "plot_tree(tree_reg1);"
      ],
      "metadata": {
        "id": "SoDgPsnCXezT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can generate the predicted values ourselves via the `predict()` method on our fitted tree model object:"
      ],
      "metadata": {
        "id": "0Kld5ztgU6Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict from the fitted tree\n",
        "tree_reg1_pred = tree_reg1.predict(X)\n",
        "tree_reg1_pred"
      ],
      "metadata": {
        "id": "dpLjfoDCVW7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make this a bit more visually appealing, we define a function to show predictions together with the underlying data:"
      ],
      "metadata": {
        "id": "Mq3fb8UIV3Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to plot data and predictions for the regression example\n",
        "def plot_reg(dfr, pred=None):\n",
        "  dfr['pred'] = pred\n",
        "  ggout = ggplot(dfr, aes(x = 'x')) + geom_point(aes(y = 'y'), alpha = 0.3) + geom_line(aes(y = 'm'), colour = 'darkgreen', size = 1.5)\n",
        "  if pred is not None:\n",
        "    ggout = ggout + geom_line(aes(y = 'pred'), colour = 'darkred', size = 1.5)\n",
        "  return(ggout)"
      ],
      "metadata": {
        "id": "fVeU0hw6DE2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we now plot the predictions we can indeed see that there are two leaf nodes present:"
      ],
      "metadata": {
        "id": "rMnQn6DDXEBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot predictions\n",
        "plot_reg(dfr, tree_reg1_pred)"
      ],
      "metadata": {
        "id": "bJgOAKh4GLUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was a very nice first step, but the decision stump is clearly too simplistic to capture the full sinusoidal patterns. Let's add one depth level to the tree:"
      ],
      "metadata": {
        "id": "g35mwiGuYg1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a DecisionTreeRegressor with max depth equal to 2\n",
        "tree_reg2 = DecisionTreeRegressor(criterion='squared_error', max_depth=2)\n",
        "# fit the tree to our data\n",
        "tree_reg2 = tree_reg2.fit(X, y)\n",
        "# plot the tree structure\n",
        "plt.figure(figsize=(8, 5), dpi=100)\n",
        "plot_tree(tree_reg2);"
      ],
      "metadata": {
        "id": "ClKeM5k-KdNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare this with the tree structure from before, what do you notice?\n",
        "\n",
        "The predictions with depth 2 and 4 leaf nodes now look as follows:"
      ],
      "metadata": {
        "id": "ZDm9c-jeZ5Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions\n",
        "tree_reg2_pred = tree_reg2.predict(X)\n",
        "# plot the predictions\n",
        "plot_reg(dfr, tree_reg2_pred)"
      ],
      "metadata": {
        "id": "yUaQzPsEPDZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Pick one of the four leaf nodes of the tree with depth two and replicate the numbers for *samples*, *value* and *squared_error*.\n",
        "* Build a tree of depth equal to 6 and check the resulting tree structure and predictions."
      ],
      "metadata": {
        "id": "xBafIdqiaPg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "node_obs = dfr.loc[dfr['x'] > 3.62]\n",
        "print(node_obs.shape[0])\n",
        "print(np.mean(node_obs.y))\n",
        "print(np.mean((node_obs.y - (np.mean(node_obs.y)))**2))"
      ],
      "metadata": {
        "id": "ZbJx1F7HPEy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "tree_reg6 = DecisionTreeRegressor(criterion='squared_error', max_depth=6)\n",
        "tree_reg6 = tree_reg6.fit(X, y)\n",
        "tree_reg6_pred = tree_reg6.predict(X)\n",
        "plot_reg(dfr, tree_reg6_pred)"
      ],
      "metadata": {
        "id": "QdXKYhnRQ6Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Parameter settings <a name=\"two-two\"></a>\n",
        "The different parameters involved in a `sklearn.tree.DecisionTreeRegressor` can be seen from the [function header](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html):\n",
        "\n",
        "*class sklearn.tree.DecisionTreeRegressor(criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)*\n",
        "\n",
        "We now fit a regression tree with parameters:\n",
        "* max leaf nodes = 100\n",
        "* max depth = 10\n",
        "* min samples split = 10\n",
        "* min samples leaf = 5\n",
        "* ccp alpha = 0\n",
        "\n",
        "This seems to lead to a severe overfit to the training data."
      ],
      "metadata": {
        "id": "vXSDl1OflfhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize, fit, predict and plot a DecisionTreeRegressor with alpha 0\n",
        "tree_reg_alpha0 = DecisionTreeRegressor(criterion='squared_error', max_leaf_nodes = 100, max_depth=10, min_samples_split=10, min_samples_leaf=5, ccp_alpha=0)\n",
        "tree_reg_alpha0 = tree_reg_alpha0.fit(X, y)\n",
        "tree_reg_alpha0_pred = tree_reg_alpha0.predict(X)\n",
        "plot_reg(dfr, tree_reg_alpha0_pred)"
      ],
      "metadata": {
        "id": "fV8OGv4sREmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the `ccp_alpha` parameter equal to zero leads to the trivial root node tree with a constant prediction for all observations:"
      ],
      "metadata": {
        "id": "16tgBbPCuZGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize, fit, predict and plot a DecisionTreeRegressor with alpha 0\n",
        "tree_reg_alpha1 = DecisionTreeRegressor(criterion='squared_error', max_leaf_nodes = 100, max_depth=10, min_samples_split=10, min_samples_leaf=5, ccp_alpha=1)\n",
        "tree_reg_alpha1 = tree_reg_alpha1.fit(X, y)\n",
        "tree_reg_alpha1_pred = tree_reg_alpha1.predict(X)\n",
        "plot_reg(dfr, tree_reg_alpha1_pred)"
      ],
      "metadata": {
        "id": "lG7EpuMBSJSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different approaches to avoid a decision tree from overfitting:\n",
        "1. One is by limiting the size of the tree via parameters like `max_leaf_nodes`, `max_depth` or `min_samples_leaf`.\n",
        "1. Another option is via the `ccp_alpha` parameter which allows to perform [minimal cost complexity pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning). In short, minimal cost complexity pruning finds the subtree that minimizes a penalized loss function where $\\alpha \\in [0,1]$ determines the penalty strength:\n",
        "  * $\\alpha = 0$ gives the biggest tree possible\n",
        "  * $\\alpha = 1$ gives the root node tree\n",
        "\n",
        "We can extract the `ccp_alpha` values from different subtrees via the `cost_complexity_pruning_path` method:"
      ],
      "metadata": {
        "id": "bRdCQUMou6fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the ccp alpha path\n",
        "cp_path = tree_reg_alpha0.cost_complexity_pruning_path(X, y)\n",
        "cp_path"
      ],
      "metadata": {
        "id": "G1Tx7zQjSPl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last item in these arrays corresponds to the trivial root node tree with the following overal MSE:"
      ],
      "metadata": {
        "id": "fQ7VRf2Q1FsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate overal MSE\n",
        "print(np.mean((y - (np.mean(y)))**2))"
      ],
      "metadata": {
        "id": "EgB2lOLc0yQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We therefore obtain all elements from the path, excluding this last element, and plot the training MSE versus the alpha values:"
      ],
      "metadata": {
        "id": "QZoT1Wxb1Q7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the alphas en MSEs\n",
        "ccp_alphas, ccp_mse = cp_path.ccp_alphas[:-1], cp_path.impurities[:-1]\n",
        "\n",
        "# plot the MSE with respect to the different values for alpha\n",
        "plt.figure(figsize=(8, 5), dpi=100)\n",
        "plt.plot(ccp_alphas, ccp_mse, marker=\"o\", drawstyle=\"steps-post\")\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"Total MSE vs effective alpha for training set\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "epB4q48PTcwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is clear that smaller alphas lead to lower MSEs on the training set and therefore to more overfitting.\n",
        "\n",
        "We now fit a regression tree to each distinct alpha value from the path as follows:"
      ],
      "metadata": {
        "id": "4kd-34Kw2B-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize an empty list to save the tree models\n",
        "tree_list = []\n",
        "# loop over the different alpha values\n",
        "for ccp_alpha in ccp_alphas:\n",
        "  # fit a regression tree\n",
        "  tree_reg = DecisionTreeRegressor(criterion='squared_error', max_leaf_nodes = 100, max_depth=10, min_samples_split=10, min_samples_leaf=5, ccp_alpha=ccp_alpha).fit(X, y)\n",
        "  # append the model to the list\n",
        "  tree_list.append(tree_reg)\n",
        "#print the length of the tree list\n",
        "len(tree_list)"
      ],
      "metadata": {
        "id": "LJmZKrXEcsxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now plot the number of nodes and the depth for the trees with different alpha values. This shows that smaller alphas lead to deeper trees with more nodes compared to those with larger values for alpha:"
      ],
      "metadata": {
        "id": "TypmViVM3gEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the node counts and depth for each tree in the list\n",
        "node_counts = [tree_reg.tree_.node_count for tree_reg in tree_list]\n",
        "depth = [tree_reg.tree_.max_depth for tree_reg in tree_list]\n",
        "\n",
        "# plot the number of nodes and depth of the trees in function of alpha\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax[0].set_xlabel(\"alpha\")\n",
        "ax[0].set_ylabel(\"number of nodes\")\n",
        "ax[0].set_title(\"Number of nodes vs alpha\")\n",
        "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax[1].set_xlabel(\"alpha\")\n",
        "ax[1].set_ylabel(\"depth of tree\")\n",
        "ax[1].set_title(\"Depth vs alpha\")\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "Xt6-dG7pfBJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now plot the predictions made by trees for different values of alpha, again showing how complexity grows for smaller alphas:"
      ],
      "metadata": {
        "id": "VzYSRJuf620C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a plot object\n",
        "plt.figure(figsize=(20, 20),dpi=100)\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.suptitle(\"Tree predictions for different alphas\", fontsize=18, y=0.95)\n",
        "# iterate over the tree list and take every 5th item\n",
        "for i, indx in enumerate(list(range(0, len(tree_list), 4))): \n",
        "#for i, indx in enumerate(list(range(len(tree_list) - 12, len(tree_list), 1))): # to zoom in on the largest alphas\n",
        "  # make a prediction for this tree\n",
        "  pred = tree_list[indx].predict(X)\n",
        "  # plot the predictions in a subplot\n",
        "  ax = plt.subplot(4, 4, i + 1)\n",
        "  ax.scatter(x,y, color='gray', s=2)\n",
        "  ax.plot(x,m,color='green')\n",
        "  ax.plot(x,pred,color='red')\n",
        "  ax.set_title(ccp_alphas[indx])\n",
        "  ax.set_xlabel('x')\n",
        "  ax.set_ylabel('y')"
      ],
      "metadata": {
        "id": "DNBvpJoUkbGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Use what you have seen so far and try to obtain a nice fit for this dataset by manually tweaking some parameters."
      ],
      "metadata": {
        "id": "evi3AkTd7n8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "tree_reg_pred = DecisionTreeRegressor(max_leaf_nodes=10, min_samples_leaf = 20).fit(X,y).predict(X)\n",
        "plot_reg(dfr, tree_reg_pred)"
      ],
      "metadata": {
        "id": "cNh93hs-7wHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Grid search cross-validation <a name=\"two-three\"></a>\n",
        "The exercise from the previous section showed us that it is possible to obtain a good model fit by manually tweaking some parameters. However, there are two very big drawbacks with that approach:\n",
        "1. This manual tweaking is time-consuming work and not fun to do.\n",
        "1. Validation of good happened on a visual basis but not in a quantitative way.\n",
        "\n",
        "A parameter grid search via cross-validation mediates both issues, giving an automatic way to try different settings and returning a quantifiable loss metric to base decisions on regarding what a \"good\" fit is. In `scikit-learn`, grid search CV is implemented in the `class sklearn.model_selection.GridSearchCV`: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
      ],
      "metadata": {
        "id": "qh-kfTyq7R6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For illustration purposes, we perform a grid search on the `ccp_alpha` parameter and keep the other settings fixed:"
      ],
      "metadata": {
        "id": "IT_OiYif906O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a parameter grid as a dict\n",
        "param_grid = {'ccp_alpha': ccp_alphas}\n",
        "# initialize the model\n",
        "tree_reg = DecisionTreeRegressor(criterion='squared_error', max_leaf_nodes = 100, max_depth=10, min_samples_split=10, min_samples_leaf=5) # note that the ccp_alpha param is not included here\n",
        "# initialize the 5-fold CV\n",
        "tree_reg_cv = GridSearchCV(tree_reg, param_grid, cv=5)\n",
        "# fit the CV\n",
        "tree_reg_cv.fit(X,y)"
      ],
      "metadata": {
        "id": "wJj8GTjlmHLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can collect the results from the `cv_results_` attribute:"
      ],
      "metadata": {
        "id": "lUplX8-x_TE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect results\n",
        "results_cv = tree_reg_cv.cv_results_\n",
        "# store in a dataframe\n",
        "results_pd = pd.DataFrame.from_dict({'alpha':ccp_alphas,'score':-results_cv['mean_test_score'],'rank':results_cv['rank_test_score']}).sort_values('rank')\n",
        "# show the top results\n",
        "results_pd.iloc[0:6]"
      ],
      "metadata": {
        "id": "RkZwYvKqn_o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now plot the predictions for the optimal alpha value according to our grid search:"
      ],
      "metadata": {
        "id": "RFBj_KdRDAqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the optimal alpha from the CV results\n",
        "opt_alpha = results_pd[results_pd['rank'] == 1]['alpha'].mean()\n",
        "# calculate the predictions for this alpha value\n",
        "pred = DecisionTreeRegressor(criterion='squared_error', max_leaf_nodes = 100, max_depth=10, min_samples_split=10, min_samples_leaf=5, ccp_alpha=opt_alpha).fit(X,y).predict(X)\n",
        "# plot the predictions\n",
        "plot_reg(dfr,pred)"
      ],
      "metadata": {
        "id": "lxrhGXSkrmxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3 - Classification tree <a name=\"three\"></a>\n",
        "A `scikit-learn` classification tree is implemented in the `sklearn.tree.DecisionTreeClassifier`: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
      ],
      "metadata": {
        "id": "5Ps2sRk2DWS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Toy example <a name=\"three-one\"></a>\n",
        "We start by fitting a simple classification tree to a toy example with simulated data. The goal is to understand the principles of how classification trees model the underlying data."
      ],
      "metadata": {
        "id": "p0-2twIrS42b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We simulate binary data in a two-dimensional plane, with two decision boundaries and some normally distributed noise on top:"
      ],
      "metadata": {
        "id": "tsoVXU0hTFtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "np.random.seed(54321)\n",
        "\n",
        "# generate the x and x2 feature vectors\n",
        "x1 = np.repeat(np.arange(0.1, 10.1, 0.1), 100)\n",
        "x2 = np.tile(np.arange(0.1, 10.1, 0.1), 100)\n",
        "\n",
        "# generate the target vector y\n",
        "y = np.zeros(len(x1), dtype=int)\n",
        "y += (x1 + 2*x2 < 8).astype(int)\n",
        "y += (3*x1 + x2 > 30).astype(int)\n",
        "y += np.round(np.random.normal(loc=0, scale=0.3, size=len(y))).astype(int)\n",
        "y = np.clip(y, 0, 1)\n",
        "\n",
        "# collect the arrays in a dataframe\n",
        "dfc = pd.DataFrame.from_dict({'x1':x1,'x2':x2,'y':y})\n",
        "# transform the y column to a category\n",
        "dfc['y'] = dfc['y'].astype(\"category\")\n",
        "\n",
        "# print the data\n",
        "dfc"
      ],
      "metadata": {
        "id": "t_Qe7aHUqkGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our simulated data looks as follows:"
      ],
      "metadata": {
        "id": "sFt3AXcpT9C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ggplot(dfc, aes(x = 'x1', y = 'x2')) + geom_point(aes(color = 'y'))"
      ],
      "metadata": {
        "id": "8vGC_PoAc0ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can start modeling, we need to create a feature matrix `X` from the arrays `x1` and `x2`:"
      ],
      "metadata": {
        "id": "GY67NAQ-UHtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stack the 1d arrays in a 2d matrix\n",
        "X = np.stack([x1,x2], axis=1)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "-APHU2Xigk9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now fit a classification tree of depth two using the `DecisionTreeClassifier` class and the `fit` method:"
      ],
      "metadata": {
        "id": "FS2CbeofUpJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a DecisionTreeClassifier of depth 2\n",
        "tree_clf2 = DecisionTreeClassifier(criterion = 'log_loss', max_depth=2)\n",
        "tree_clf2.fit(X,y)"
      ],
      "metadata": {
        "id": "hqQwXPIRdy3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the resulting tree structure via the `sklearn.tree.plot_tree` function:"
      ],
      "metadata": {
        "id": "3d1QllZiVNp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the tree structure\n",
        "plt.figure(figsize=(8, 5), dpi=100)\n",
        "plot_tree(tree_clf2);"
      ],
      "metadata": {
        "id": "2EJLIYMchAJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make predictions of the fitted classification tree for:\n",
        "* the predicted class label via `predict()`\n",
        "* the predicted class probabilities via `predict_proba()`.\n",
        "\n",
        "Below we compute both and assign them to the `dfc` dataframe."
      ],
      "metadata": {
        "id": "eWE3AWkziiLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the probabilities\n",
        "dfc['phat'] = tree_clf2.predict_proba(X)[:,1]\n",
        "# predict the class labels\n",
        "dfc['yhat'] = tree_clf2.predict(X)\n",
        "# convert both to categories for plotting\n",
        "dfc['yhat'] = dfc['yhat'].astype('category')\n",
        "dfc['phat'] = dfc['phat'].astype('category')"
      ],
      "metadata": {
        "id": "2zJDX07QhLrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted class labels look like this:"
      ],
      "metadata": {
        "id": "oHA714SqjAfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot yhat\n",
        "ggplot(dfc, aes(x = 'x1', y = 'x2')) + geom_point(aes(color = 'yhat'))"
      ],
      "metadata": {
        "id": "Asp0iN6ghlXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the predicted class probabilities look like this:"
      ],
      "metadata": {
        "id": "nRk0cIzmjGNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot phat\n",
        "ggplot(dfc, aes(x = 'x1', y = 'x2')) + geom_point(aes(color = 'phat'))"
      ],
      "metadata": {
        "id": "2uJJQaNIh7LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Pick one of the four leaf nodes of the tree with depth two and replicate the numbers for *samples*, *value* and *log_loss*.\n",
        "* Bonus: can you also explain the predicted probability *phat* for this group?\n",
        "* Build a tree of depth equal to 20 and check the resulting tree structure and predictions."
      ],
      "metadata": {
        "id": "70YUVGThWTgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "node_obs = dfc.query('x1 > 8.15 & x2 > 4.55')\n",
        "print(node_obs.shape[0])\n",
        "print(node_obs.query('y == 0').shape[0])\n",
        "print(node_obs.query('y == 1').shape[0])\n",
        "print(node_obs.query('y == 1').shape[0] / node_obs.shape[0])"
      ],
      "metadata": {
        "id": "cfiIJyAhXq59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "from sklearn.metrics import log_loss\n",
        "y_obs = node_obs.y.to_numpy()\n",
        "X_obs = np.stack([node_obs.x1.to_numpy(),node_obs.x2.to_numpy()], axis=1)\n",
        "p_obs = tree_clf2.predict_proba(X_obs)\n",
        "\n",
        "log_loss0 = log_loss(y_obs,p_obs[:,1])\n",
        "log_loss1 = -np.mean((y_obs * np.log(p_obs[:,1])) + ((1-y_obs) * np.log(p_obs[:,0])))\n",
        "log_loss2 = -np.mean((y_obs * np.log2(p_obs[:,1])) + ((1-y_obs) * np.log2(p_obs[:,0])))\n",
        "print(log_loss0)\n",
        "print(log_loss1)\n",
        "print(log_loss2)"
      ],
      "metadata": {
        "id": "siwzFoPaYxZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "count_0 = node_obs.query('y == 0').shape[0] / node_obs.shape[0]\n",
        "count_1 = node_obs.query('y == 1').shape[0] / node_obs.shape[0]\n",
        "-(count_0 * np.log2(count_0) + count_1 * np.log2(count_1))"
      ],
      "metadata": {
        "id": "i7ei7yXjVRZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "tree_clf20 = DecisionTreeClassifier(criterion='log_loss', max_depth=20).fit(X, y)\n",
        "dfc['yhat'] = tree_clf20.predict(X)\n",
        "dfc['yhat'] = dfc['yhat'].astype('category')\n",
        "ggplot(dfc, aes(x = 'x1', y = 'x2')) + geom_point(aes(color = 'yhat'))"
      ],
      "metadata": {
        "id": "4qMPi1ATWa9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Grid search cross-validation <a name=\"three-two\"></a>\n",
        "We now perform a simple grid search over one parameter with cross-validation to find the optimal classification tree."
      ],
      "metadata": {
        "id": "87pyKSLrl3w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a grid for the max leaf nodes\n",
        "param_grid_clf = {'max_leaf_nodes': list(range(2,100))}\n",
        "# initialize the classifier\n",
        "tree_clf = DecisionTreeClassifier()\n",
        "# initialize the grid search\n",
        "tree_clf_cv = GridSearchCV(tree_clf, param_grid_clf, cv=5, scoring='f1')\n",
        "# fit the cross-validation\n",
        "tree_clf_cv.fit(X,y)"
      ],
      "metadata": {
        "id": "0idfqKB1iuIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the CV results\n",
        "results_clf_cv = tree_clf_cv.cv_results_\n",
        "# collect in a dataframe\n",
        "results_clf_pd = pd.DataFrame.from_dict({'size':param_grid_clf['max_leaf_nodes'],'score':results_clf_cv['mean_test_score'],'rank':results_clf_cv['rank_test_score']}).sort_values('rank')\n",
        "# inspect the top results\n",
        "results_clf_pd.iloc[0:6]"
      ],
      "metadata": {
        "id": "grCw_zfbjU_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the optimal tuning parameter value\n",
        "opt_size_clf = results_clf_pd[results_clf_pd['rank'] == 1]['size'].min().astype(int)\n",
        "# fit and predict from the optimal tree\n",
        "pred_clf = DecisionTreeClassifier(max_leaf_nodes=opt_size_clf).fit(X,y).predict(X)\n",
        "# plot the predicted class labels\n",
        "dfc['yhat'] = pred_clf\n",
        "dfc['yhat'] = dfc['yhat'].astype('category')\n",
        "ggplot(dfc, aes(x = 'x1', y = 'x2')) + geom_point(aes(color = 'yhat'))"
      ],
      "metadata": {
        "id": "lFoH3MOJkGOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* What do you think of the \"optimal\" classifier?\n",
        "* Expand the grid search via the code above to include more tuning parameters and obtain a better classifier."
      ],
      "metadata": {
        "id": "S1tXFXVrrgiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "dfc['yhat'] = DecisionTreeClassifier(max_leaf_nodes=75).fit(X,y).predict(X)\n",
        "dfc['yhat'] = dfc['yhat'].astype('category')\n",
        "ggplot(dfc, aes(x = 'x1', y = 'x2')) + geom_point(aes(color = 'yhat'))"
      ],
      "metadata": {
        "id": "W1tofEfysC_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4 - Actuarial tree <a name=\"four\"></a>\n",
        "So far we have seen how decision trees can be applied to classical regression and classification problems. Now it is time to tackle more actuarial applications with decision trees."
      ],
      "metadata": {
        "id": "fTJ5er4oDvA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 MTPL data <a name=\"four-one\"></a>\n",
        "We will use a Belgian motor third party liability (MTPL) dataset to illustrate how decision trees can be used for insurance pricing applications. Let's start by reading and preparing the data in a `pandas` dataframe:"
      ],
      "metadata": {
        "id": "1Et7hjwhK35p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the MTPL data\n",
        "mtpl = pd.read_csv(\"https://katrienantonio.github.io/hands-on-machine-learning-R-module-1/data/PC_data.txt\", delimiter = \"\\t\", usecols=list(range(1,14)))\n",
        "# transform the column names to lowercase\n",
        "mtpl.columns = mtpl.columns.str.lower()\n",
        "# rename the exp column to expo\n",
        "mtpl = mtpl.rename(columns= {'exp': 'expo'})\n",
        "# print the shape\n",
        "print(mtpl.shape)\n",
        "# show the first observations\n",
        "mtpl.head(100)"
      ],
      "metadata": {
        "id": "TPLye6c0k5Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our columns have different types, namely integers, floats and objects (which can be seen as strings in our case):"
      ],
      "metadata": {
        "id": "65F4acR3OpOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get types of all the columns\n",
        "mtpl.dtypes"
      ],
      "metadata": {
        "id": "pY1bnO7Jnp2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ML applications, we need to transform all our columns to a numerical input. We will transform our categorical features to integers:"
      ],
      "metadata": {
        "id": "TbpxwnltO7yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map string values to integers for certain columns\n",
        "mtpl['coverage'] = mtpl['coverage'].map({'TPL':0, 'PO':1, 'FO':2})\n",
        "mtpl['fleet'] = mtpl['fleet'].map({'N':0, 'Y':1})\n",
        "mtpl['fuel'] = mtpl['fuel'].map({'gasoline':0, 'diesel':1})\n",
        "mtpl['use'] = mtpl['use'].map({'private':0, 'work':1})\n",
        "mtpl['sex'] = mtpl['sex'].map({'male':0, 'female':1})\n",
        "\n",
        "# check whether the types have changed\n",
        "mtpl.dtypes"
      ],
      "metadata": {
        "id": "7y6wdtqSnpz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our MTPL data in numerical format now looks like this:"
      ],
      "metadata": {
        "id": "zIGH1gfTRMYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show the first observations\n",
        "mtpl.head(100)"
      ],
      "metadata": {
        "id": "aIKxvfmDnpq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Claim frequency <a name=\"four-two\"></a>\n",
        "We start our modeling efforts by building a claim frequency `DecisionTreeRegressor` model for the MTPL dataset. The [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) teaches us that there is a `poisson criterion` which uses reduction in Poisson deviance to find splits, which sounds great for our use-case."
      ],
      "metadata": {
        "id": "RVuMo6dPU3Mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create our feature matrix with all the MTPL features:"
      ],
      "metadata": {
        "id": "2ViSLZtoVO31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cols to retain as features\n",
        "feat_cols = ['bm','ageph','agec','power','coverage','fuel','sex','fleet','use']\n",
        "# subset the data\n",
        "X_mtpl_freq = mtpl[feat_cols]\n",
        "# print the shape\n",
        "print(X_mtpl_freq.shape)\n",
        "# show the features\n",
        "X_mtpl_freq"
      ],
      "metadata": {
        "id": "evlnRFp_nxwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create our target and weight arrays from the number of claims and exposure:"
      ],
      "metadata": {
        "id": "xX40FQMBYwU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# claim frequency (nclaims/expo) as target\n",
        "y_mtpl_freq = np.array(mtpl.nclaims/mtpl.expo)\n",
        "# exposure as weights\n",
        "w_mtpl_freq = np.array(mtpl.expo)"
      ],
      "metadata": {
        "id": "ML2dHIucnxu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we fit our claim frequency regression tree with the `poisson` criterion:"
      ],
      "metadata": {
        "id": "WyNJV7ls0vtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a tree of depth 2\n",
        "tree_freq = DecisionTreeRegressor(criterion='poisson', max_depth=2, min_samples_split=10000, min_samples_leaf=5000)\n",
        "# fit the tree to our target with weights\n",
        "tree_freq.fit(X=X_mtpl_freq, y=y_mtpl_freq, sample_weight=w_mtpl_freq)\n",
        "# print the tree\n",
        "tree_freq"
      ],
      "metadata": {
        "id": "08QC3tMcnLfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the tree structure and make it more readable by supplying feature names via the `feature_names` parameter:"
      ],
      "metadata": {
        "id": "FMFypEec1d6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the tree structure\n",
        "plt.figure(figsize=(8, 5), dpi=100)\n",
        "plot_tree(tree_freq, feature_names=feat_cols);"
      ],
      "metadata": {
        "id": "_AIe8dXZ1dE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a step back and see whether the prediction of 0.139 makes sense for the root node. This would be the overall claim frequency for the entire portfolio, which we can calculate as:"
      ],
      "metadata": {
        "id": "85JLfG1R2Kbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# empirical claim frequency portfolio\n",
        "np.sum(mtpl.nclaims) / np.sum(mtpl.expo)"
      ],
      "metadata": {
        "id": "p6kMl89E265x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This checks out, nice! Given that we are doing weighted regression, this is calculated as follows by `sklearn`:"
      ],
      "metadata": {
        "id": "pM68DbCe3F-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weighted overall prediction\n",
        "np.sum(y_mtpl_freq * w_mtpl_freq)/np.sum(w_mtpl_freq)"
      ],
      "metadata": {
        "id": "WnYDKYzYqK3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that our specification of the target and weights makes sense from a portfolio perspective.\n",
        "\n",
        "Let's now calculate the numbers for the bottom right node with `bm > 10.5`, starting with the number of samples:"
      ],
      "metadata": {
        "id": "vmqNUVgo3aZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subset the mtpl data\n",
        "mtpl_subset = mtpl.query('bm > 10.5')\n",
        "# get the number of samples\n",
        "mtpl_subset.shape[0]"
      ],
      "metadata": {
        "id": "POgQFtRrZSjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will calculate the prediction value via the `predict` method:"
      ],
      "metadata": {
        "id": "JMQUuFbT4_tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict for the subset of policyholders\n",
        "tree_freq_pred = tree_freq.predict(mtpl_subset[feat_cols])\n",
        "# take the unique values\n",
        "np.unique(tree_freq_pred)"
      ],
      "metadata": {
        "id": "0v91MsIJpEYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this prediction is an unweighted version, i.e., an **annual claim frequency** for someone in this group. This can be seen as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "bWNlmqcF5db9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(mtpl_subset['nclaims'])/np.sum(mtpl_subset['expo'])"
      ],
      "metadata": {
        "id": "UvYbZ0XRrG7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given this prediction, we can now calculate the Poisson deviance with `sklearn.metrics.mean_poisson_deviance`:"
      ],
      "metadata": {
        "id": "XkK1zfv-6AoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the Poisson deviance\n",
        "from sklearn.metrics import mean_poisson_deviance\n",
        "mean_poisson_deviance(mtpl_subset.eval('nclaims/expo'), tree_freq_pred, sample_weight = mtpl_subset.eval('expo'))/2"
      ],
      "metadata": {
        "id": "0wY7Z9t-toU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three important things to note here:\n",
        "1. both `y_true` and `y_pred` are expressed in an annual basis\n",
        "1. the exposure metric is supplied as a weight\n",
        "1. the `DecisionTreeRegressor` class implements half the Poisson deviance as an impurity measure ([GitHub code](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_criterion.pyx))"
      ],
      "metadata": {
        "id": "NqfRKWnN6p7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Experiment with the settings to obtain a more detailed idea of which features are driving the claim frequency\n",
        "* At what point is the tree not interpretable enough? We'll tackle this issue in the next section, stay tuned!"
      ],
      "metadata": {
        "id": "rJI9sD2v78Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "param_grid_freq = {'max_leaf_nodes': list(range(3,20))}\n",
        "tree_freq = DecisionTreeRegressor(criterion='poisson')\n",
        "tree_freq_cv = GridSearchCV(tree_freq, param_grid_freq, cv=5, scoring='neg_mean_poisson_deviance')\n",
        "tree_freq_cv.fit(X_mtpl_freq, y=y_mtpl_freq, sample_weight=w_mtpl_freq)"
      ],
      "metadata": {
        "id": "TA91MAgSfcGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add ypur code here\n",
        "results_freq_cv = tree_freq_cv.cv_results_\n",
        "results_freq_pd = pd.DataFrame.from_dict({'size':param_grid_freq['max_leaf_nodes'],'score':results_freq_cv['mean_test_score'],'rank':results_freq_cv['rank_test_score']}).sort_values('rank')\n",
        "results_freq_pd.iloc[0:6]"
      ],
      "metadata": {
        "id": "Ly88I3448u6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "opt_size_freq = results_freq_pd[results_freq_pd['rank'] == 1]['size'].min().astype(int)\n",
        "tree_freq = DecisionTreeRegressor(criterion='poisson', max_leaf_nodes=opt_size_freq).fit(X_mtpl_freq, y_mtpl_freq, w_mtpl_freq)\n",
        "plt.figure(figsize=(20, 10), dpi=100)\n",
        "plot_tree(tree_freq, feature_names=feat_cols);"
      ],
      "metadata": {
        "id": "oYFAoCYS-XTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Claim severity <a name=\"four-three\"></a>\n",
        "We continue our modeling efforts by building a claim severity `DecisionTreeRegressor` model for the MTPL dataset. The [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) teaches us, unfortunately, that there is no suitable criterion for long-tailed distributions like claim severity. So what can we do?"
      ],
      "metadata": {
        "id": "1u3gZBfhAa4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first try to model the log-transformed version of claim severity with a MSE criterion and exponentiate the results afterwards, a little bit in line with how we fit log-normal GLMs."
      ],
      "metadata": {
        "id": "qZ1kKPpUB6Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first subset the data keeping only the observations with actual claims (within a certain range for simplicity):"
      ],
      "metadata": {
        "id": "KUfya-1RCXWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subset the data based on claim amount\n",
        "mtpl_sev = mtpl.query('amount > 1 & amount < 100000')\n",
        "mtpl_sev.shape"
      ],
      "metadata": {
        "id": "umxtmCtMBUr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we extract the features, target (average claim size) and weights (the number of claims):"
      ],
      "metadata": {
        "id": "wuphrto-DojC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features\n",
        "X_mtpl_sev = mtpl_sev[feat_cols]\n",
        "# target and log-transformed version\n",
        "y_mtpl_sev = np.array(mtpl_sev.avg)\n",
        "y_mtpl_sev_log = np.log(y_mtpl_sev)\n",
        "# weights\n",
        "w_mtpl_sev = np.array(mtpl_sev.nclaims)"
      ],
      "metadata": {
        "id": "0ae08Ws4BUpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit a MSE regression tree of depth one to the log-transformed target:"
      ],
      "metadata": {
        "id": "XPL4QA8ZEQpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a tree\n",
        "tree_sev_log = DecisionTreeRegressor(criterion='squared_error', max_depth=1).fit(X_mtpl_sev, y_mtpl_sev_log, w_mtpl_sev)\n",
        "# plot the tree\n",
        "plot_tree(tree_sev_log, feature_names=feat_cols);"
      ],
      "metadata": {
        "id": "gpFT7bKtEPaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the root node indeed predicts the weighted claim severity on the log scale for the entire portfolio:"
      ],
      "metadata": {
        "id": "iOsaNjEUVopN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weighted mean of the protfolio log severity\n",
        "np.sum(w_mtpl_sev * y_mtpl_sev_log) / np.sum(w_mtpl_sev)"
      ],
      "metadata": {
        "id": "GtkAzZLbVpI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about the claim severity predictions on the actual or non-log scale? When we exponentiate the prediction result of the root node, we notice we are closer to the empirical median as to the mean severity:"
      ],
      "metadata": {
        "id": "Z6QYnh1nG1wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exponent of the root node prediction\n",
        "print(np.exp(6.15))\n",
        "# overall mean\n",
        "print(np.mean(mtpl_sev.avg))\n",
        "# overall median\n",
        "print(np.median(mtpl_sev.avg))"
      ],
      "metadata": {
        "id": "3ZIRCoEKG1aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be explained in two different ways:\n",
        "1. The mean of the log-normal distribution is equal to $\\exp(\\mu + \\sigma^2/2)$ and we are calculating $\\exp(\\mu)$, which is the median. But unlike GLMs, we do not get a proper estimate for $\\sigma$ in a decision tree, only an estimate for $\\mu$.\n",
        "2. The exponential of an average is not equal to the average of an exponential: $\\exp(1/n\\sum_1^n x_i) \\neq 1/n \\sum_i^n\\exp(x_i)$, and by exponentiating after the root node prediction we are doing the first, while we should be doing the latter."
      ],
      "metadata": {
        "id": "LKaN5LYGHLy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach we can take is by simply modeling the claim severity with a MSE regression tree as follows:"
      ],
      "metadata": {
        "id": "lnNZp-v8HLqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a tree\n",
        "tree_sev = DecisionTreeRegressor(criterion='squared_error', max_depth=1).fit(X_mtpl_sev, y_mtpl_sev, w_mtpl_sev)\n",
        "# plot the tree\n",
        "plot_tree(tree_sev, feature_names=feat_cols);"
      ],
      "metadata": {
        "id": "SR6_kMXrFFdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though this is not a valid statistical assumption typically, it leads to a correct estimation of the average claim severity in the root node:"
      ],
      "metadata": {
        "id": "dpdGe_VjI5kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overall claim severity\n",
        "np.sum(mtpl_sev.amount) / np.sum(mtpl_sev.nclaims)"
      ],
      "metadata": {
        "id": "AVVJ5F1MFuBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# weighted overall prediction\n",
        "np.sum(y_mtpl_sev * w_mtpl_sev)/np.sum(w_mtpl_sev)"
      ],
      "metadata": {
        "id": "EQTl7eXHF6I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, there is not an ideal way to model claim severities with a decision tree in `scikit-learn` because the distributional loss functions are not implemented (yet?). Later on, we will see some ensemble methods where this is the case however, so stay tuned."
      ],
      "metadata": {
        "id": "4G-SrOlpJWPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5 - Interpretation tools <a name=\"five\"></a>\n",
        "Decision trees are typically considered as explainable models given their simple structure. However, inferring interpretations might become difficult for complex trees with a deep structure. We therefore introduce some interpretation tools that can assist to understand your model better. These are model-agnostic tools, meaning that they can be applied to any type of ML model. Let's use the following claim frequency tree as an example:"
      ],
      "metadata": {
        "id": "0bZ0D2MdJu2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a moderately sized frequency tree and plot the structure\n",
        "tree_freq = DecisionTreeRegressor(criterion='poisson', max_leaf_nodes=25, min_samples_leaf = 1000).fit(X_mtpl_freq, y_mtpl_freq, w_mtpl_freq)\n",
        "plt.figure(figsize=(20, 10), dpi=100)\n",
        "plot_tree(tree_freq, feature_names=feat_cols);"
      ],
      "metadata": {
        "id": "qKrNzcjYYDtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can youn tell which features are driving the claim frequency prediction result? And can you explain how certain features relate to the prediction target? If not, no worries, the following two tools will assist you with exactly those questions."
      ],
      "metadata": {
        "id": "3gQgM-hRZBPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Feature importance <a name=\"five-one\"></a>\n",
        "The feature importance metric explains how important each feature is in your ML model, simple right? In `sklearn` these values are an attribute of your fitted model object, namely the `feature_importances_` attribute:"
      ],
      "metadata": {
        "id": "oormmPwpKcMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the feature importance values\n",
        "tree_freq.feature_importances_"
      ],
      "metadata": {
        "id": "MHALt4eUJQl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the bonus-malus feature accounts for over 80% of the predictive power in the model, followed by the age of the policyholder and power of the car with respectively 10% and 5%. The features `sex`, `fleet` and `use` are not used in the tree and have zero importance:"
      ],
      "metadata": {
        "id": "Ud_85TBpclK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the feature names and importance scores\n",
        "tree_freq_fi = pd.DataFrame({'feature':tree_freq.feature_names_in_, 'importance':tree_freq.feature_importances_}).sort_values('importance', ascending=False)\n",
        "# inspect the results\n",
        "tree_freq_fi"
      ],
      "metadata": {
        "id": "vEq4tTQ0Kg4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The importance of a feature is computed as the total reduction of the criterion, in our case the Poisson deviance, brought by that feature. In the end, these values are normalized to sum to one over all features.\n",
        "\n",
        "This approach can however be misleading for high cardinality features with many unique values, as these have more split options to start with. Impurity-based feature importance scores can therefore give biased results with an over/underestimation of the importance for high/low cardinality features.\n",
        "\n",
        "An alternative solution is a permutation-based importance score, which is calculated as follows. First, a baseline loss metric is evaluated on the benchmark dataset. Next, a feature column from that dataset is randomly permuted (shuffled) and the metric is evaluated again. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. The larger the drop, the more important the feature is.\n",
        "\n",
        "Let's test this approach out via the `sklearn.inspection.permutation_importance` function ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html)):\n"
      ],
      "metadata": {
        "id": "Pe0z8MWNdSEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the permutation importance\n",
        "perm_imp = permutation_importance(tree_freq, X_mtpl_freq, y_mtpl_freq, sample_weight=w_mtpl_freq, scoring='neg_mean_poisson_deviance', n_repeats=5, random_state=0, max_samples=1.0)\n",
        "perm_imp"
      ],
      "metadata": {
        "id": "hftr7fZRKhdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get get multiple importance scores per feature (defined by `n_repeats`) so we can extract the average score per feature and normalize it to sum over one for all the different features:"
      ],
      "metadata": {
        "id": "X0wezyP8kfMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the average\n",
        "pi = perm_imp['importances_mean']\n",
        "# normalize and add to results\n",
        "tree_freq_fi['permutation'] = pi / np.sum(pi)\n",
        "# show results\n",
        "tree_freq_fi"
      ],
      "metadata": {
        "id": "SX7I6zGZKhZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that both importance measures are very similar, but that the higher cardinality features like `bm`, `ageph`, `power` and `agec` lose some importance to the lower cardinality features like `fuel` and `coverage`.\n",
        "\n",
        "We now know which features are driving the predictions for our claim frequency tree, but how do they do it?"
      ],
      "metadata": {
        "id": "3uBhKhS4lATe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Partial dependence <a name=\"five-two\"></a>\n",
        "A partial dependence plot (PDP) quantifies and shows the relation between one ore more features and the prediction target. The partial dependence corresponds to the average prediction of a model for each possible value of the feature. In `sklearn` we have two options to generate PDPs:\n",
        "1. Create a built-in display graph directly from the fitted model object via the `sklearn.inspection.PartialDependenceDisplay.from_estimator` function ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html#sklearn.inspection.PartialDependenceDisplay.from_estimator)).\n",
        "2. Calculate PD data via the `sklearn.inspection.partial_dependence` function ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.partial_dependence.html#sklearn.inspection.partial_dependence)) and create a custom plot.\n",
        "\n",
        "Let's start with the first approach and create a built-in PDP for some features:"
      ],
      "metadata": {
        "id": "ICU2bLWFKhx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create pdps for a couple of features\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "PartialDependenceDisplay.from_estimator(tree_freq, X_mtpl_freq, features = ['bm','ageph','power','fuel','agec','coverage'], categorical_features=['fuel','coverage'], kind='average', ax=ax);"
      ],
      "metadata": {
        "id": "xfbMkE_gbAqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to visualize interaction effects of two features by supplying a tuple to the `features` parameter:"
      ],
      "metadata": {
        "id": "-WZ3IudkGslL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create 2D PDP\n",
        "PartialDependenceDisplay.from_estimator(tree_freq, X_mtpl_freq, features = [('ageph','power')], kind='average');"
      ],
      "metadata": {
        "id": "IxuJuPRWGTsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we calculate the partial dependence data to do some custom plots:"
      ],
      "metadata": {
        "id": "W_-BFBXc6OLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the pd for bm\n",
        "tree_pd_bm = partial_dependence(tree_freq, X_mtpl_freq, features = ['bm'], percentiles=(0.05, 0.95), grid_resolution=100, kind='average')\n",
        "tree_pd_bm"
      ],
      "metadata": {
        "id": "x_y2SdKnKkx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the dict to a pandas daatframe\n",
        "tree_pd_bm['average'] = tree_pd_bm['average'][0]\n",
        "tree_pd_bm['values'] = tree_pd_bm['values'][0]\n",
        "tree_pd_bm = pd.DataFrame.from_dict(tree_pd_bm).rename(columns={'average':'pd','values':'bm'})\n",
        "tree_pd_bm"
      ],
      "metadata": {
        "id": "-8rk9-25HUD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom PD ggplot\n",
        "ggplot(tree_pd_bm, aes(x = 'bm')) + geom_line(aes(y = 'pd'), colour = 'darkblue', size = 1)"
      ],
      "metadata": {
        "id": "Fnrnc0KbHecz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!**\n",
        "\n",
        "* Feel free to experiment with the interpretation tools to explore the tree or create a cool custom graph.\n",
        "* Try to replicate the PD effect for a specific value of a feature of choice."
      ],
      "metadata": {
        "id": "mvg1Os9yJ5X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "partial_dependence(tree_freq, X_mtpl_freq, features = ['bm'], method='brute', kind='average')"
      ],
      "metadata": {
        "id": "sY712s5AN8B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "X_mtpl_freq_adj = X_mtpl_freq.copy()\n",
        "X_mtpl_freq_adj['bm'] = 1\n",
        "X_mtpl_freq_adj"
      ],
      "metadata": {
        "id": "FY7jGFxEQD6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here\n",
        "tree_freq.predict(X_mtpl_freq_adj).mean()"
      ],
      "metadata": {
        "id": "hf3xsAEcS2_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}